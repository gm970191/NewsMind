#!/usr/bin/env python3
"""
è°ƒè¯•æµè§ˆå™¨å¯åŠ¨å’Œå†…å®¹æå–
"""
import sys
import asyncio
from pathlib import Path

# æ·»åŠ backendç›®å½•åˆ°Pythonè·¯å¾„
backend_dir = Path(__file__).parent.parent / "backend"
sys.path.insert(0, str(backend_dir))

from app.services.crawler import WebCrawler
from app.services.news_service import NewsRepository
from app.core.database import SessionLocal

async def debug_browser():
    """è°ƒè¯•æµè§ˆå™¨å¯åŠ¨"""
    print("ğŸ” è°ƒè¯•æµè§ˆå™¨å¯åŠ¨...")
    
    db = SessionLocal()
    repo = NewsRepository(db)
    
    try:
        # åˆ›å»ºçˆ¬è™«å®ä¾‹
        crawler = WebCrawler(repo)
        print(f"âœ… çˆ¬è™«åˆ›å»ºæˆåŠŸ")
        print(f"   Playwright Available: {crawler._playwright_available}")
        print(f"   Browser: {crawler.browser}")
        print(f"   Page: {crawler.page}")
        
        # å¯åŠ¨æµè§ˆå™¨
        print("\nğŸš€ å¯åŠ¨æµè§ˆå™¨...")
        await crawler.start_browser()
        print(f"âœ… æµè§ˆå™¨å¯åŠ¨å®Œæˆ")
        print(f"   Playwright Available: {crawler._playwright_available}")
        print(f"   Browser: {crawler.browser}")
        print(f"   Page: {crawler.page}")
        
        if crawler.page:
            print("\nğŸ”— æµ‹è¯•é¡µé¢è®¿é—®...")
            try:
                # ä½¿ç”¨ä¸€ä¸ªæ›´ç®€å•çš„ç½‘ç«™è¿›è¡Œæµ‹è¯•
                await crawler.page.goto('https://httpbin.org/html', timeout=30000)
                title = await crawler.page.title()
                print(f"âœ… é¡µé¢è®¿é—®æˆåŠŸï¼Œæ ‡é¢˜: {title}")
                
                # æµ‹è¯•å†…å®¹æå–
                print("\nğŸ“„ æµ‹è¯•å†…å®¹æå–...")
                content = await crawler._get_full_content('https://httpbin.org/html')
                if content:
                    print(f"âœ… å†…å®¹æå–æˆåŠŸ")
                    print(f"   å†…å®¹é•¿åº¦: {len(content)} å­—ç¬¦")
                    print(f"   å†…å®¹é¢„è§ˆ: {content[:200]}...")
                else:
                    print("âŒ å†…å®¹æå–å¤±è´¥")
                    
            except Exception as e:
                print(f"âŒ é¡µé¢è®¿é—®å¤±è´¥: {e}")
        else:
            print("âŒ æµè§ˆå™¨é¡µé¢æœªåˆ›å»º")
        
        # å…³é—­æµè§ˆå™¨
        await crawler.close_browser()
        print("\nâœ… æµè§ˆå™¨å·²å…³é—­")
        
    except Exception as e:
        print(f"âŒ è°ƒè¯•å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
    finally:
        db.close()

if __name__ == "__main__":
    asyncio.run(debug_browser()) 
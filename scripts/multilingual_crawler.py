#!/usr/bin/env python3
"""
å¤šè¯­è¨€æ–°é—»çˆ¬è™« - æ”¯æŒè‹±è¯­ã€æ—¥è¯­ã€éŸ©è¯­ã€ä¸­æ–‡ç­‰
"""
import sqlite3
import time
import json
from datetime import datetime
import urllib.request
import urllib.parse
import re

def get_news_sources():
    """è·å–æ‰€æœ‰æ´»è·ƒçš„æ–°é—»æº"""
    conn = sqlite3.connect("newsmind.db")
    cursor = conn.cursor()
    
    try:
        cursor.execute("""
            SELECT id, name, url, type, category, is_active
            FROM news_sources 
            WHERE is_active = 1
            ORDER BY id
        """)
        sources = cursor.fetchall()
        return sources
    finally:
        conn.close()

def detect_language_advanced(title, content, source_name):
    """é«˜çº§è¯­è¨€æ£€æµ‹"""
    # åŸºäºæ–°é—»æºåç§°çš„è¯­è¨€æ˜ å°„
    language_mapping = {
        # è‹±è¯­æ–°é—»æº
        'en': ['CNN', 'BBC News', 'Reuters', 'TechCrunch', 'Bloomberg', 
               'The Guardian', 'The New York Times', 'NPR News', 'Ars Technica', 'Wired',
               'Google News', 'BBC World', 'Al Jazeera', 'France 24', 'Deutsche Welle',
               'æµ·å³¡æ—¶æŠ¥', 'æ›¼è°·é‚®æŠ¥', 'å°åº¦æ—¶æŠ¥', 'é©¬æ¥è¥¿äºšæ˜ŸæŠ¥'],
        
        # æ—¥è¯­æ–°é—»æº
        'ja': ['æœæ—¥æ–°é—»', 'è¯»å–æ–°é—»', 'æ—¥æœ¬ç»æµæ–°é—»'],
        
        # éŸ©è¯­æ–°é—»æº
        'ko': ['éŸ©å›½ä¸­å¤®æ—¥æŠ¥', 'éŸ©å›½ç»æµæ—¥æŠ¥'],
        
        # ä¸­æ–‡æ–°é—»æº
        'zh': ['æ–°æµªæ–°é—»', 'è…¾è®¯æ–°é—»', 'ç½‘æ˜“æ–°é—»', 'å‡¤å‡°ç½‘', 'æ¾æ¹ƒæ–°é—»', '36æ°ª', 'è™å—…ç½‘', 'é’›åª’ä½“', 'æ–°åŠ å¡æ—©æŠ¥']
    }
    
    # é¦–å…ˆåŸºäºæ–°é—»æºåç§°åˆ¤æ–­
    for lang, sources in language_mapping.items():
        if source_name in sources:
            return lang
    
    # åŸºäºå†…å®¹ç‰¹å¾åˆ¤æ–­
    text = (title + " " + content).lower()
    
    # æ—¥è¯­ç‰¹å¾
    japanese_chars = re.findall(r'[\u3040-\u309f\u30a0-\u30ff\u4e00-\u9faf]', text)
    if len(japanese_chars) > len(text) * 0.1:
        return 'ja'
    
    # éŸ©è¯­ç‰¹å¾
    korean_chars = re.findall(r'[\uac00-\ud7af]', text)
    if len(korean_chars) > len(text) * 0.1:
        return 'ko'
    
    # ä¸­æ–‡ç‰¹å¾
    chinese_chars = re.findall(r'[\u4e00-\u9fff]', text)
    if len(chinese_chars) > len(text) * 0.3:
        return 'zh'
    
    # é»˜è®¤è‹±è¯­
    return 'en'

def fetch_url_content(url):
    """è·å–URLå†…å®¹"""
    try:
        headers = {
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36'
        }
        req = urllib.request.Request(url, headers=headers)
        with urllib.request.urlopen(req, timeout=30) as response:
            return response.read().decode('utf-8')
    except Exception as e:
        print(f"     âŒ è·å–å†…å®¹å¤±è´¥: {e}")
        return None

def parse_rss_content(content, source_name):
    """è§£æRSSå†…å®¹"""
    articles = []
    
    # ç®€å•çš„RSSè§£æ
    title_pattern = r'<title>(.*?)</title>'
    link_pattern = r'<link>(.*?)</link>'
    description_pattern = r'<description>(.*?)</description>'
    
    titles = re.findall(title_pattern, content, re.DOTALL)
    links = re.findall(link_pattern, content, re.DOTALL)
    descriptions = re.findall(description_pattern, content, re.DOTALL)
    
    # è¿‡æ»¤æ‰RSS feedæœ¬èº«çš„æ ‡é¢˜
    if titles and 'rss' in titles[0].lower():
        titles = titles[1:]
    
    for i in range(min(len(titles), len(links), 10)):  # æœ€å¤š10ç¯‡
        try:
            title = re.sub(r'<[^>]+>', '', titles[i]).strip()
            link = links[i].strip()
            description = re.sub(r'<[^>]+>', '', descriptions[i]).strip() if i < len(descriptions) else ""
            
            if title and link and not title.startswith('http'):
                # é«˜çº§è¯­è¨€æ£€æµ‹
                language = detect_language_advanced(title, description, source_name)
                
                articles.append({
                    'title': title,
                    'content': description,
                    'source_url': link,
                    'publish_time': datetime.now(),
                    'language': language
                })
        except Exception as e:
            continue
    
    return articles

def save_articles(articles, source_id, source_name, category):
    """ä¿å­˜æ–‡ç« åˆ°æ•°æ®åº“"""
    if not articles:
        return 0
    
    conn = sqlite3.connect("newsmind.db")
    cursor = conn.cursor()
    
    saved_count = 0
    
    try:
        for article in articles:
            # æ£€æŸ¥æ˜¯å¦å·²å­˜åœ¨ï¼ˆåŸºäºURLå»é‡ï¼‰
            cursor.execute("SELECT id FROM news_articles WHERE source_url = ?", (article['source_url'],))
            if cursor.fetchone():
                continue
            
            # æ’å…¥æ–°æ–‡ç« 
            cursor.execute("""
                INSERT INTO news_articles (
                    title, content, source_name, source_url, category, language,
                    publish_time, created_at, updated_at, is_processed, source_id
                ) VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?)
            """, (
                article['title'],
                article['content'],
                source_name,
                article['source_url'],
                category,
                article['language'],
                article['publish_time'].isoformat(),
                datetime.now().isoformat(),
                datetime.now().isoformat(),
                0,  # æœªå¤„ç†
                source_id
            ))
            
            saved_count += 1
        
        conn.commit()
        
    except Exception as e:
        print(f"     âŒ ä¿å­˜æ–‡ç« å¤±è´¥: {e}")
        conn.rollback()
    finally:
        conn.close()
    
    return saved_count

def crawl_multilingual_news():
    """çˆ¬å–å¤šè¯­è¨€æ–°é—»"""
    print("ğŸš€ å¼€å§‹çˆ¬å–å¤šè¯­è¨€æ–°é—»...")
    print("=" * 60)
    
    sources = get_news_sources()
    total_new_articles = 0
    language_stats = {'en': 0, 'ja': 0, 'ko': 0, 'zh': 0}
    
    for source in sources:
        source_id, name, url, source_type, category, is_active = source
        
        print(f"\nğŸ“° æ­£åœ¨çˆ¬å–: {name}")
        print(f"   URL: {url}")
        
        try:
            if source_type == 'rss':
                content = fetch_url_content(url)
                if content:
                    articles = parse_rss_content(content, name)
                    saved_count = save_articles(articles, source_id, name, category)
                    
                    # ç»Ÿè®¡è¯­è¨€åˆ†å¸ƒ
                    for article in articles:
                        lang = article['language']
                        if lang in language_stats:
                            language_stats[lang] += 1
                    
                    print(f"   âœ… æˆåŠŸä¿å­˜ {saved_count} ç¯‡æ–°æ–‡ç« ")
                    total_new_articles += saved_count
                else:
                    print(f"   âŒ è·å–å†…å®¹å¤±è´¥")
            else:
                print(f"   âš ï¸  æš‚ä¸æ”¯æŒ {source_type} ç±»å‹")
                
        except Exception as e:
            print(f"   âŒ çˆ¬å–å¤±è´¥: {e}")
    
    return total_new_articles, language_stats

def main():
    """ä¸»å‡½æ•°"""
    print("NewsMind å¤šè¯­è¨€æ–°é—»çˆ¬è™«")
    print("=" * 60)
    
    # å¼€å§‹çˆ¬å–
    start_time = time.time()
    new_articles, language_stats = crawl_multilingual_news()
    end_time = time.time()
    
    print("\n" + "=" * 60)
    print("ğŸ“Š çˆ¬å–ç»“æœ")
    print("=" * 60)
    print(f"æ–°å¢æ–‡ç« : {new_articles}")
    print(f"è€—æ—¶: {end_time - start_time:.2f} ç§’")
    
    print(f"\nğŸŒ è¯­è¨€åˆ†å¸ƒ:")
    print(f"  è‹±è¯­ (en): {language_stats['en']} ç¯‡")
    print(f"  æ—¥è¯­ (ja): {language_stats['ja']} ç¯‡")
    print(f"  éŸ©è¯­ (ko): {language_stats['ko']} ç¯‡")
    print(f"  ä¸­æ–‡ (zh): {language_stats['zh']} ç¯‡")
    
    if new_articles > 0:
        print(f"\nğŸ‰ æˆåŠŸè·å– {new_articles} ç¯‡å¤šè¯­è¨€æ–°é—»!")
        print("ğŸ“ ç°åœ¨å¯ä»¥æµ‹è¯•å¤§æ¨¡å‹çš„æ€»ç»“ä¸ç¿»è¯‘èƒ½åŠ›:")
        print("   - è‹±è¯­æ–°é—»ç¿»è¯‘ä¸ºä¸­æ–‡")
        print("   - æ—¥è¯­æ–°é—»ç¿»è¯‘ä¸ºä¸­æ–‡")
        print("   - éŸ©è¯­æ–°é—»ç¿»è¯‘ä¸ºä¸­æ–‡")
        print("   - å¤šè¯­è¨€æ–°é—»æ™ºèƒ½æ€»ç»“")
    else:
        print(f"\nâš ï¸  æœªè·å–åˆ°æ–°æ–‡ç« ")

if __name__ == "__main__":
    main() 
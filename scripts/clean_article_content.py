#!/usr/bin/env python3
"""
æ¸…ç†æ–‡ç« å†…å®¹ï¼Œç§»é™¤HTMLæ ‡ç­¾å¹¶ä¼˜åŒ–å†…å®¹è´¨é‡
"""
import sys
import re
from pathlib import Path

# æ·»åŠ backendç›®å½•åˆ°Pythonè·¯å¾„
backend_dir = Path(__file__).parent.parent / "backend"
sys.path.insert(0, str(backend_dir))

from app.core.database import get_db
from app.models.news import NewsArticle
from bs4 import BeautifulSoup

def clean_content(content):
    """æ¸…ç†æ–‡ç« å†…å®¹"""
    if not content:
        return content
    
    # ä½¿ç”¨BeautifulSoupç§»é™¤HTMLæ ‡ç­¾
    soup = BeautifulSoup(content, 'html.parser')
    text = soup.get_text()
    
    # æ¸…ç†å¤šä½™çš„ç©ºç™½å­—ç¬¦
    text = re.sub(r'\s+', ' ', text).strip()
    
    # ç§»é™¤å¸¸è§çš„æ— ç”¨æ–‡æœ¬
    text = re.sub(r'^(Read more|Continue reading|More info|Learn more)', '', text, flags=re.IGNORECASE)
    
    return text

def optimize_article_content():
    """ä¼˜åŒ–æ–‡ç« å†…å®¹"""
    print("ğŸ§¹ å¼€å§‹æ¸…ç†å’Œä¼˜åŒ–æ–‡ç« å†…å®¹...")
    
    db = next(get_db())
    
    try:
        # è·å–æ‰€æœ‰æ–‡ç« 
        articles = db.query(NewsArticle).all()
        
        cleaned_count = 0
        improved_count = 0
        
        for article in articles:
            original_content = article.original_content
            if not original_content:
                continue
            
            # æ¸…ç†å†…å®¹
            cleaned_content = clean_content(original_content)
            
            # å¦‚æœæ¸…ç†åçš„å†…å®¹æ›´çŸ­ï¼Œè¯´æ˜åŸæ¥æœ‰HTMLæ ‡ç­¾
            if len(cleaned_content) < len(original_content):
                article.original_content = cleaned_content
                cleaned_count += 1
                print(f"âœ… æ¸…ç†æ–‡ç«  {article.id}: {len(original_content)} -> {len(cleaned_content)} å­—ç¬¦")
            
            # å¦‚æœå†…å®¹å¤ªçŸ­ï¼ˆå°‘äº50å­—ç¬¦ï¼‰ï¼Œå°è¯•ç”¨æ ‡é¢˜è¡¥å……
            if len(cleaned_content) < 50 and article.original_title:
                enhanced_content = f"{article.original_title}. {cleaned_content}"
                if len(enhanced_content) > len(cleaned_content):
                    article.original_content = enhanced_content
                    improved_count += 1
                    print(f"ğŸ“ å¢å¼ºæ–‡ç«  {article.id}: {len(cleaned_content)} -> {len(enhanced_content)} å­—ç¬¦")
        
        # æäº¤æ›´æ”¹
        db.commit()
        
        print(f"\nğŸ“Š æ¸…ç†ç»“æœ:")
        print(f"   æ¸…ç†HTMLæ ‡ç­¾: {cleaned_count} ç¯‡")
        print(f"   å†…å®¹å¢å¼º: {improved_count} ç¯‡")
        print(f"   æ€»è®¡å¤„ç†: {cleaned_count + improved_count} ç¯‡")
        
        # æ˜¾ç¤ºæ¸…ç†åçš„ç»Ÿè®¡
        print(f"\nğŸ“ˆ æ¸…ç†åçš„å†…å®¹é•¿åº¦ç»Ÿè®¡:")
        short_count = 0
        medium_count = 0
        long_count = 0
        
        for article in articles:
            content_length = len(article.original_content or "")
            if content_length < 100:
                short_count += 1
            elif content_length < 500:
                medium_count += 1
            else:
                long_count += 1
        
        print(f"   çŸ­æ–‡ç«  (<100å­—ç¬¦): {short_count} ç¯‡")
        print(f"   ä¸­ç­‰æ–‡ç«  (100-500å­—ç¬¦): {medium_count} ç¯‡")
        print(f"   é•¿æ–‡ç«  (â‰¥500å­—ç¬¦): {long_count} ç¯‡")
        print(f"   æ€»è®¡: {len(articles)} ç¯‡")
        
    except Exception as e:
        print(f"âŒ æ¸…ç†å¤±è´¥: {e}")
        db.rollback()
    finally:
        db.close()

if __name__ == "__main__":
    optimize_article_content() 
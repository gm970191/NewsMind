#!/usr/bin/env python3
"""
ç½‘é¡µæ­£æ–‡æå–æ¨¡å—
ä»æ–°é—»åŸæ–‡é“¾æ¥ä¸­æå–å®Œæ•´æ­£æ–‡å†…å®¹
"""
import requests
from bs4 import BeautifulSoup
import re
import time
import sys
import os
sys.path.insert(0, os.path.dirname(__file__))
from news_filter import clean_html_tags

def extract_article_content(url, max_retries=2):
    """
    ä»æ–°é—»é“¾æ¥ä¸­æå–æ­£æ–‡å†…å®¹
    """
    headers = {
        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'
    }
    
    for attempt in range(max_retries):
        try:
            response = requests.get(url, headers=headers, timeout=15)
            response.raise_for_status()
            
            soup = BeautifulSoup(response.content, 'html.parser')
            
            # ç§»é™¤scriptå’Œstyleæ ‡ç­¾
            for script in soup(["script", "style"]):
                script.decompose()
            
            # å°è¯•å¤šç§æ­£æ–‡æå–ç­–ç•¥
            content = extract_content_by_strategy(soup, url)
            
            if content and len(content.strip()) > 100:
                return clean_html_tags(content)
            
            time.sleep(1)  # é¿å…è¯·æ±‚è¿‡å¿«
            
        except Exception as e:
            print(f"     âš ï¸  æå–å¤±è´¥ (å°è¯• {attempt + 1}/{max_retries}): {e}")
            if attempt < max_retries - 1:
                time.sleep(2)
            continue
    
    return None

def extract_content_by_strategy(soup, url):
    """
    ä½¿ç”¨å¤šç§ç­–ç•¥æå–æ­£æ–‡å†…å®¹
    """
    # ç­–ç•¥1: æŸ¥æ‰¾å¸¸è§çš„æ­£æ–‡å®¹å™¨
    content_selectors = [
        'article',
        '.article-content',
        '.story-content',
        '.post-content',
        '.entry-content',
        '.content-body',
        '.article-body',
        '.story-body',
        '.post-body',
        '.entry-body',
        '[class*="content"]',
        '[class*="article"]',
        '[class*="story"]',
        '[class*="post"]',
        '[class*="entry"]'
    ]
    
    for selector in content_selectors:
        elements = soup.select(selector)
        for element in elements:
            # ç§»é™¤å¯¼èˆªã€å¹¿å‘Šç­‰æ— å…³å†…å®¹
            for unwanted in element.select('nav, .nav, .navigation, .ad, .advertisement, .sidebar, .comments'):
                unwanted.decompose()
            
            text = element.get_text(separator=' ', strip=True)
            if len(text) > 200:  # ç¡®ä¿å†…å®¹è¶³å¤Ÿé•¿
                return text
    
    # ç­–ç•¥2: æŸ¥æ‰¾åŒ…å«æœ€å¤šæ–‡æœ¬çš„æ®µè½
    paragraphs = soup.find_all('p')
    if paragraphs:
        # æŒ‰é•¿åº¦æ’åºï¼Œå–æœ€é•¿çš„å‡ ä¸ªæ®µè½
        paragraphs.sort(key=lambda p: len(p.get_text()), reverse=True)
        content_parts = []
        total_length = 0
        
        for p in paragraphs[:10]:  # æœ€å¤šå–10ä¸ªæ®µè½
            text = p.get_text(strip=True)
            if len(text) > 50:  # æ®µè½è‡³å°‘50å­—ç¬¦
                content_parts.append(text)
                total_length += len(text)
                if total_length > 500:  # æ€»é•¿åº¦è¶…è¿‡500å­—ç¬¦å°±åœæ­¢
                    break
        
        if content_parts:
            return ' '.join(content_parts)
    
    # ç­–ç•¥3: åŸºäºURLç‰¹å¾çš„ç‰¹æ®Šå¤„ç†
    if 'bbc.com' in url:
        return extract_bbc_content(soup)
    elif 'cnn.com' in url:
        return extract_cnn_content(soup)
    elif 'reuters.com' in url:
        return extract_reuters_content(soup)
    elif 'nytimes.com' in url:
        return extract_nytimes_content(soup)
    
    return None

def extract_bbc_content(soup):
    """BBCç‰¹å®šæå–é€»è¾‘"""
    content = soup.find('div', {'data-component': 'text-block'})
    if content:
        return content.get_text(separator=' ', strip=True)
    return None

def extract_cnn_content(soup):
    """CNNç‰¹å®šæå–é€»è¾‘"""
    content = soup.find('div', class_='l-container')
    if content:
        paragraphs = content.find_all('p', class_='paragraph')
        if paragraphs:
            return ' '.join([p.get_text(strip=True) for p in paragraphs])
    return None

def extract_reuters_content(soup):
    """Reutersç‰¹å®šæå–é€»è¾‘"""
    content = soup.find('div', class_='article-body')
    if content:
        return content.get_text(separator=' ', strip=True)
    return None

def extract_nytimes_content(soup):
    """NYTimesç‰¹å®šæå–é€»è¾‘"""
    content = soup.find('section', {'name': 'articleBody'})
    if content:
        return content.get_text(separator=' ', strip=True)
    return None

def enhance_rss_article(article):
    """
    å¢å¼ºRSSæ–‡ç« å†…å®¹
    å¦‚æœRSSåªæä¾›æ‘˜è¦ï¼Œå°è¯•ä»åŸæ–‡é“¾æ¥æå–å®Œæ•´å†…å®¹
    """
    if not article.get('source_url'):
        return article
    
    # å¦‚æœå†…å®¹å·²ç»è¶³å¤Ÿé•¿ï¼Œä¸éœ€è¦æå–
    if len(article.get('content', '')) > 300:
        return article
    
    print(f"     ğŸ” å°è¯•æå–å®Œæ•´å†…å®¹: {article['title'][:50]}...")
    
    extracted_content = extract_article_content(article['source_url'])
    if extracted_content:
        article['content'] = extracted_content
        print(f"     âœ… æˆåŠŸæå–ï¼Œå†…å®¹é•¿åº¦: {len(extracted_content)}")
    else:
        print(f"     âš ï¸  æå–å¤±è´¥ï¼Œä¿æŒåŸå†…å®¹")
    
    return article

if __name__ == '__main__':
    # æµ‹è¯•æå–åŠŸèƒ½
    test_url = "https://www.bbc.com/news/world-us-canada-123456"
    print("æµ‹è¯•ç½‘é¡µå†…å®¹æå–...")
    content = extract_article_content(test_url)
    if content:
        print(f"æå–æˆåŠŸï¼Œå†…å®¹é•¿åº¦: {len(content)}")
        print(f"å†…å®¹é¢„è§ˆ: {content[:200]}...")
    else:
        print("æå–å¤±è´¥") 
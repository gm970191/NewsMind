#!/usr/bin/env python3
"""
é‡‡é›†æŒ‡å®šæ–°é—»æºçš„çœŸå®æ•°æ®
"""
import requests
import sqlite3
from datetime import datetime
import xml.etree.ElementTree as ET
from urllib.parse import urljoin
import time

def crawl_specific_sources():
    """é‡‡é›†æŒ‡å®šæ–°é—»æº"""
    conn = sqlite3.connect("backend/newsmind.db")
    cursor = conn.cursor()
    
    # æ–°é—»æºé…ç½®
    sources = [
        {
            "name": "Al Jazeera",
            "description": "ä¸­ä¸œè§†è§’ã€å…¨çƒå—æ–¹å›½å®¶é‡è¦åª’ä½“",
            "url": "https://www.aljazeera.com/xml/rss/all.xml",
            "category": "å›½é™…"
        },
        {
            "name": "The Washington Post",
            "description": "ç¾å›½æ”¿ç•ŒæŠ¥é“ä¸»åŠ›åª’ä½“", 
            "url": "https://feeds.washingtonpost.com/rss/world",
            "category": "å›½é™…"
        }
    ]
    
    total_articles = 0
    
    try:
        print("ğŸŒ å¼€å§‹é‡‡é›†æŒ‡å®šæ–°é—»æº...")
        print("=" * 60)
        
        for source in sources:
            print(f"\nğŸ“° æ­£åœ¨é‡‡é›†: {source['name']}")
            print(f"   æè¿°: {source['description']}")
            print(f"   URL: {source['url']}")
            
            try:
                # è·å–RSSå†…å®¹
                headers = {
                    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'
                }
                response = requests.get(source['url'], headers=headers, timeout=10)
                response.raise_for_status()
                
                # è§£æRSS
                root = ET.fromstring(response.content)
                
                # æŸ¥æ‰¾æ‰€æœ‰itemå…ƒç´ 
                items = root.findall('.//item')
                print(f"   æ‰¾åˆ° {len(items)} ç¯‡æ–‡ç« ")
                
                articles_saved = 0
                for i, item in enumerate(items[:10]):  # é™åˆ¶æ¯æºæœ€å¤š10ç¯‡
                    try:
                        # æå–æ–‡ç« ä¿¡æ¯
                        title_elem = item.find('title')
                        link_elem = item.find('link')
                        description_elem = item.find('description')
                        pub_date_elem = item.find('pubDate')
                        
                        if title_elem is None or title_elem.text is None:
                            continue
                            
                        title = title_elem.text.strip()
                        link = link_elem.text.strip() if link_elem is not None and link_elem.text else ""
                        description = description_elem.text.strip() if description_elem is not None and description_elem.text else ""
                        
                        # å¤„ç†å‘å¸ƒæ—¶é—´
                        pub_date = None
                        if pub_date_elem is not None and pub_date_elem.text:
                            try:
                                pub_date = datetime.strptime(pub_date_elem.text, '%a, %d %b %Y %H:%M:%S %z')
                            except:
                                pub_date = datetime.now()
                        else:
                            pub_date = datetime.now()
                        
                        # ç”Ÿæˆå”¯ä¸€URL
                        unique_url = f"{link}_{i}" if link else f"https://{source['name'].lower().replace(' ', '')}.com/article_{i}"
                        
                        # ä¿å­˜åˆ°æ•°æ®åº“
                        cursor.execute("""
                            INSERT INTO news_articles 
                            (title, source_name, source_url, source_id, category, language, content, content_length,
                             publish_time, created_at, updated_at, is_processed)
                            VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?)
                        """, (
                            title,
                            source['name'],
                            unique_url,
                            total_articles + 1,
                            source['category'],
                            'en',  # è¿™äº›æºéƒ½æ˜¯è‹±æ–‡
                            description or title,  # å¦‚æœæ²¡æœ‰æè¿°å°±ç”¨æ ‡é¢˜
                            len(description or title),
                            pub_date.isoformat(),
                            datetime.now().isoformat(),
                            datetime.now().isoformat(),
                            False
                        ))
                        
                        articles_saved += 1
                        print(f"   âœ“ ä¿å­˜: {title[:50]}...")
                        
                    except Exception as e:
                        print(f"   âŒ å¤„ç†æ–‡ç« å¤±è´¥: {e}")
                        continue
                
                print(f"   âœ… æˆåŠŸä¿å­˜ {articles_saved} ç¯‡æ–‡ç« ")
                total_articles += articles_saved
                
            except Exception as e:
                print(f"   âŒ é‡‡é›†å¤±è´¥: {e}")
                continue
            
            # é¿å…è¯·æ±‚è¿‡å¿«
            time.sleep(2)
        
        conn.commit()
        print(f"\nğŸ‰ é‡‡é›†å®Œæˆï¼æ€»å…±ä¿å­˜ {total_articles} ç¯‡æ–‡ç« ")
        
        # æ˜¾ç¤ºæœ€æ–°æ–‡ç« 
        cursor.execute("""
            SELECT title, source_name, created_at 
            FROM news_articles 
            ORDER BY created_at DESC 
            LIMIT 5
        """)
        
        latest_articles = cursor.fetchall()
        print(f"\nğŸ“° æœ€æ–°æ–‡ç« :")
        for article in latest_articles:
            title, source, created_at = article
            print(f"   - {title} ({source})")
        
    except Exception as e:
        print(f"âŒ é‡‡é›†è¿‡ç¨‹å‡ºé”™: {e}")
        conn.rollback()
    finally:
        conn.close()

if __name__ == "__main__":
    crawl_specific_sources() 
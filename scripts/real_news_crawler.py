#!/usr/bin/env python3
"""
çœŸå®æ–°é—»çˆ¬è™« - æ”¯æŒå¤šç§æ–°é—»æº
"""
import sys
import os
import time
import json
import requests
import sqlite3
from datetime import datetime, timedelta
from pathlib import Path
from urllib.parse import urljoin, urlparse
import re

# æ·»åŠ backendç›®å½•åˆ°Pythonè·¯å¾„
backend_dir = Path(__file__).parent.parent / "backend"
sys.path.insert(0, str(backend_dir))

try:
    import feedparser
except ImportError:
    print("å®‰è£…feedparser: pip install feedparser")
    sys.exit(1)

class RealNewsCrawler:
    """çœŸå®æ–°é—»çˆ¬è™«"""
    
    def __init__(self):
        self.db_path = "newsmind.db"
        self.session = requests.Session()
        self.session.headers.update({
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'
        })
        self.results = {
            'total_sources': 0,
            'success_count': 0,
            'error_count': 0,
            'new_articles': 0,
            'errors': []
        }
    
    def get_news_sources(self):
        """è·å–æ‰€æœ‰æ´»è·ƒçš„æ–°é—»æº"""
        conn = sqlite3.connect(self.db_path)
        cursor = conn.cursor()
        
        try:
            cursor.execute("""
                SELECT id, name, url, type, category, is_active
                FROM news_sources 
                WHERE is_active = 1
                ORDER BY id
            """)
            sources = cursor.fetchall()
            return sources
        finally:
            conn.close()
    
    def crawl_all_sources(self):
        """çˆ¬å–æ‰€æœ‰æ–°é—»æº"""
        sources = self.get_news_sources()
        self.results['total_sources'] = len(sources)
        
        print(f"ğŸš€ å¼€å§‹çˆ¬å– {len(sources)} ä¸ªæ–°é—»æº...")
        print("=" * 60)
        
        for source in sources:
            source_id, name, url, source_type, category, is_active = source
            print(f"\nğŸ“° æ­£åœ¨çˆ¬å–: {name} ({source_type})")
            print(f"   URL: {url}")
            
            try:
                if source_type == 'rss':
                    articles = self.crawl_rss_source(source)
                elif source_type == 'api':
                    articles = self.crawl_api_source(source)
                else:
                    print(f"  âš ï¸  ä¸æ”¯æŒçš„æºç±»å‹: {source_type}")
                    continue
                
                # ä¿å­˜æ–‡ç« 
                saved_count = self.save_articles(articles, source)
                print(f"  âœ… æˆåŠŸä¿å­˜ {saved_count} ç¯‡æ–°æ–‡ç« ")
                self.results['success_count'] += 1
                
            except Exception as e:
                error_msg = f"çˆ¬å– {name} å¤±è´¥: {str(e)}"
                print(f"  âŒ {error_msg}")
                self.results['error_count'] += 1
                self.results['errors'].append(error_msg)
        
        return self.results
    
    def crawl_rss_source(self, source):
        """çˆ¬å–RSSæº"""
        source_id, name, url, source_type, category, is_active = source
        
        try:
            # è·å–RSSå†…å®¹
            response = self.session.get(url, timeout=30)
            response.raise_for_status()
            
            # è§£æRSS
            feed = feedparser.parse(response.content)
            articles = []
            
            print(f"   ğŸ“Š æ‰¾åˆ° {len(feed.entries)} ç¯‡æ–‡ç« ")
            
            for entry in feed.entries[:20]:  # é™åˆ¶æœ€å¤š20ç¯‡
                try:
                    # æå–æ–‡ç« ä¿¡æ¯
                    title = entry.get('title', '').strip()
                    content = entry.get('summary', '').strip()
                    link = entry.get('link', '')
                    
                    if not title or not link:
                        continue
                    
                    # è§£æå‘å¸ƒæ—¶é—´
                    publish_time = None
                    if entry.get('published'):
                        try:
                            publish_time = datetime(*entry.published_parsed[:6])
                        except:
                            publish_time = datetime.now()
                    else:
                        publish_time = datetime.now()
                    
                    # æ¸…ç†å†…å®¹
                    content = self.clean_content(content)
                    
                    article_data = {
                        'title': title,
                        'content': content,
                        'source_url': link,
                        'source_id': source_id,
                        'source_name': name,
                        'publish_time': publish_time,
                        'category': category,
                        'language': 'zh' if self.is_chinese(title) else 'en'
                    }
                    
                    articles.append(article_data)
                    
                except Exception as e:
                    print(f"     âš ï¸  è§£ææ–‡ç« å¤±è´¥: {e}")
                    continue
            
            return articles
            
        except Exception as e:
            raise Exception(f"RSSè§£æå¤±è´¥: {e}")
    
    def crawl_api_source(self, source):
        """çˆ¬å–APIæº"""
        source_id, name, url, source_type, category, is_active = source
        
        try:
            # è·å–APIæ•°æ®
            response = self.session.get(url, timeout=30)
            response.raise_for_status()
            
            data = response.json()
            articles = []
            
            # æ–°æµªæ–°é—»APIæ ¼å¼
            if 'sina' in url.lower():
                if 'result' in data and 'data' in data['result']:
                    items = data['result']['data']
                    print(f"   ğŸ“Š æ‰¾åˆ° {len(items)} ç¯‡æ–‡ç« ")
                    
                    for item in items[:20]:
                        try:
                            title = item.get('title', '').strip()
                            content = item.get('intro', '').strip()
                            link = item.get('url', '')
                            
                            if not title or not link:
                                continue
                            
                            # è§£æå‘å¸ƒæ—¶é—´
                            publish_time = datetime.now()
                            if item.get('ctime'):
                                try:
                                    publish_time = datetime.fromtimestamp(int(item['ctime']))
                                except:
                                    pass
                            
                            article_data = {
                                'title': title,
                                'content': content,
                                'source_url': link,
                                'source_id': source_id,
                                'source_name': name,
                                'publish_time': publish_time,
                                'category': category,
                                'language': 'zh'
                            }
                            
                            articles.append(article_data)
                            
                        except Exception as e:
                            print(f"     âš ï¸  è§£ææ–‡ç« å¤±è´¥: {e}")
                            continue
            
            return articles
            
        except Exception as e:
            raise Exception(f"APIè§£æå¤±è´¥: {e}")
    
    def clean_content(self, content):
        """æ¸…ç†å†…å®¹"""
        if not content:
            return ""
        
        # ç§»é™¤HTMLæ ‡ç­¾
        content = re.sub(r'<[^>]+>', '', content)
        
        # ç§»é™¤å¤šä½™ç©ºç™½
        content = re.sub(r'\s+', ' ', content).strip()
        
        # é™åˆ¶é•¿åº¦
        if len(content) > 1000:
            content = content[:1000] + "..."
        
        return content
    
    def is_chinese(self, text):
        """åˆ¤æ–­æ˜¯å¦ä¸ºä¸­æ–‡"""
        if not text:
            return False
        chinese_chars = re.findall(r'[\u4e00-\u9fff]', text)
        return len(chinese_chars) > len(text) * 0.3
    
    def save_articles(self, articles, source):
        """ä¿å­˜æ–‡ç« åˆ°æ•°æ®åº“"""
        if not articles:
            return 0
        
        conn = sqlite3.connect(self.db_path)
        cursor = conn.cursor()
        
        saved_count = 0
        
        try:
            for article in articles:
                # æ£€æŸ¥æ˜¯å¦å·²å­˜åœ¨ï¼ˆåŸºäºURLå»é‡ï¼‰
                cursor.execute("SELECT id FROM news_articles WHERE source_url = ?", (article['source_url'],))
                if cursor.fetchone():
                    continue
                
                # æ’å…¥æ–°æ–‡ç« 
                cursor.execute("""
                    INSERT INTO news_articles (
                        title, content, source_name, source_url, category, language,
                        publish_time, created_at, updated_at, is_processed, source_id
                    ) VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?)
                """, (
                    article['title'],
                    article['content'],
                    article['source_name'],
                    article['source_url'],
                    article['category'],
                    article['language'],
                    article['publish_time'].isoformat(),
                    datetime.now().isoformat(),
                    datetime.now().isoformat(),
                    0,  # æœªå¤„ç†
                    article['source_id']
                ))
                
                saved_count += 1
                self.results['new_articles'] += 1
            
            conn.commit()
            
        except Exception as e:
            print(f"     âŒ ä¿å­˜æ–‡ç« å¤±è´¥: {e}")
            conn.rollback()
        finally:
            conn.close()
        
        return saved_count
    
    def print_results(self):
        """æ‰“å°çˆ¬å–ç»“æœ"""
        print("\n" + "=" * 60)
        print("ğŸ“Š çˆ¬å–ç»“æœç»Ÿè®¡")
        print("=" * 60)
        print(f"æ€»æ–°é—»æº: {self.results['total_sources']}")
        print(f"æˆåŠŸçˆ¬å–: {self.results['success_count']}")
        print(f"çˆ¬å–å¤±è´¥: {self.results['error_count']}")
        print(f"æ–°å¢æ–‡ç« : {self.results['new_articles']}")
        
        if self.results['errors']:
            print(f"\nâŒ é”™è¯¯è¯¦æƒ…:")
            for error in self.results['errors']:
                print(f"  - {error}")

def main():
    """ä¸»å‡½æ•°"""
    print("NewsMind çœŸå®æ–°é—»çˆ¬è™«")
    print("=" * 60)
    
    crawler = RealNewsCrawler()
    
    # å¼€å§‹çˆ¬å–
    start_time = time.time()
    results = crawler.crawl_all_sources()
    end_time = time.time()
    
    # æ‰“å°ç»“æœ
    crawler.print_results()
    
    print(f"\nâ±ï¸  è€—æ—¶: {end_time - start_time:.2f} ç§’")
    
    if results['new_articles'] > 0:
        print(f"\nğŸ‰ æˆåŠŸè·å– {results['new_articles']} ç¯‡æ–°æ–‡ç« !")
        print("ğŸ“ ç°åœ¨å¯ä»¥è®¿é—®å‰ç«¯é¡µé¢æŸ¥çœ‹æœ€æ–°æ–°é—»")
    else:
        print(f"\nâš ï¸  æœªè·å–åˆ°æ–°æ–‡ç« ï¼Œå¯èƒ½åŸå› :")
        print("   - æ–‡ç« å·²å­˜åœ¨ï¼ˆå»é‡æœºåˆ¶ï¼‰")
        print("   - æ–°é—»æºæš‚æ—¶æ— æ›´æ–°")
        print("   - ç½‘ç»œè¿æ¥é—®é¢˜")

if __name__ == "__main__":
    main() 
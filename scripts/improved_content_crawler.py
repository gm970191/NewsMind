#!/usr/bin/env python3
"""
æ”¹è¿›çš„å†…å®¹çˆ¬è™«è„šæœ¬
ä¸“é—¨ç”¨äºŽæŠ“å–æœ‰å®Œæ•´å†…å®¹çš„æ–°é—»
"""
import sys
import asyncio
import os
from pathlib import Path

# æ·»åŠ backendç›®å½•åˆ°Pythonè·¯å¾„
backend_dir = Path(__file__).parent.parent / "backend"
sys.path.insert(0, str(backend_dir))

from app.core.database import SessionLocal
from app.services.news_service import NewsRepository
from app.services.crawler import WebCrawler
from app.models.news import NewsSource


async def crawl_with_full_content():
    """æŠ“å–æœ‰å®Œæ•´å†…å®¹çš„æ–°é—»"""
    print("ðŸ” å¼€å§‹æŠ“å–æœ‰å®Œæ•´å†…å®¹çš„æ–°é—»...")
    
    db = SessionLocal()
    repo = NewsRepository(db)
    
    try:
        # åˆ›å»ºä¸€äº›é«˜è´¨é‡æ–°é—»æº
        quality_sources = [
            {
                'name': 'BBC News',
                'url': 'https://www.bbc.com/news',
                'type': 'web',
                'category': 'å›½é™…',
                'weight': 1.0,
                'is_active': True
            },
            {
                'name': 'Reuters',
                'url': 'https://www.reuters.com',
                'type': 'web',
                'category': 'å›½é™…',
                'weight': 1.0,
                'is_active': True
            },
            {
                'name': 'TechCrunch',
                'url': 'https://techcrunch.com',
                'type': 'web',
                'category': 'ç§‘æŠ€',
                'weight': 1.0,
                'is_active': True
            },
            {
                'name': 'CNN',
                'url': 'https://www.cnn.com',
                'type': 'web',
                'category': 'å›½é™…',
                'weight': 1.0,
                'is_active': True
            }
        ]
        
        # æ·»åŠ é«˜è´¨é‡æ–°é—»æº
        for source_data in quality_sources:
            try:
                # æ£€æŸ¥æ˜¯å¦å·²å­˜åœ¨
                existing = repo.get_source_by_url(source_data['url'])
                if not existing:
                    repo.create_source(source_data)
                    print(f"âœ“ æ·»åŠ æ–°é—»æº: {source_data['name']}")
                else:
                    print(f"âš  æ–°é—»æºå·²å­˜åœ¨: {source_data['name']}")
            except Exception as e:
                print(f"âœ— æ·»åŠ æ–°é—»æºå¤±è´¥: {e}")
        
        # ä½¿ç”¨æ”¹è¿›çš„çˆ¬è™«æŠ“å–æ–°é—»
        print("\nðŸš€ å¼€å§‹æ–°é—»é‡‡é›†...")
        async with WebCrawler(repo) as crawler:
            results = await crawler.crawl_news_sources()
            
            print(f"\nðŸ“Š é‡‡é›†ç»“æžœ:")
            print(f"   æ€»æ–°é—»æº: {results['total_sources']}")
            print(f"   æˆåŠŸ: {results['success_count']}")
            print(f"   å¤±è´¥: {results['error_count']}")
            print(f"   æ–°æ–‡ç« : {results['new_articles']}")
        
        # æ˜¾ç¤ºé‡‡é›†åˆ°çš„æ–‡ç« è¯¦æƒ…
        print("\nðŸ“° é‡‡é›†åˆ°çš„æ–‡ç« è¯¦æƒ…:")
        articles = repo.get_articles(limit=20, order_by='created_at DESC')
        for article in articles:
            content_length = len(article.original_content) if article.original_content else 0
            print(f"   ID: {article.id}")
            print(f"   æ ‡é¢˜: {article.original_title}")
            print(f"   æ¥æº: {article.source_name}")
            print(f"   å†…å®¹é•¿åº¦: {content_length} å­—ç¬¦")
            if content_length > 0:
                content_preview = article.original_content[:100] + "..." if content_length > 100 else article.original_content
                print(f"   å†…å®¹é¢„è§ˆ: {content_preview}")
            else:
                print(f"   å†…å®¹é¢„è§ˆ: [æ— å†…å®¹]")
            print("-" * 50)
        
        # ç»Ÿè®¡å†…å®¹é•¿åº¦åˆ†å¸ƒ
        print("\nðŸ“ˆ å†…å®¹é•¿åº¦ç»Ÿè®¡:")
        short_articles = [a for a in articles if len(a.original_content or '') < 100]
        medium_articles = [a for a in articles if 100 <= len(a.original_content or '') < 500]
        long_articles = [a for a in articles if len(a.original_content or '') >= 500]
        
        print(f"   çŸ­æ–‡ç«  (<100å­—ç¬¦): {len(short_articles)} ç¯‡")
        print(f"   ä¸­ç­‰æ–‡ç«  (100-500å­—ç¬¦): {len(medium_articles)} ç¯‡")
        print(f"   é•¿æ–‡ç«  (â‰¥500å­—ç¬¦): {len(long_articles)} ç¯‡")
        
    except Exception as e:
        print(f"âŒ æŠ“å–å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
    finally:
        db.close()


def main():
    """ä¸»å‡½æ•°"""
    print("ðŸš€ æ”¹è¿›çš„å†…å®¹çˆ¬è™«")
    print("=" * 50)
    
    # è¿è¡ŒæŠ“å–
    asyncio.run(crawl_with_full_content())
    
    print("\nâœ… æŠ“å–å®Œæˆï¼")


if __name__ == "__main__":
    main() 
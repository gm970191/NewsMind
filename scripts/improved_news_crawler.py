#!/usr/bin/env python3
"""
æ”¹è¿›çš„æ–°é—»é‡‡é›†æµ‹è¯•è„šæœ¬
ç”¨äºæµ‹è¯•çœŸå®æ–°é—»æºçš„é‡‡é›†å’Œå†…å®¹è·å–
"""
import sys
import asyncio
import os
from pathlib import Path

# æ·»åŠ backendç›®å½•åˆ°Pythonè·¯å¾„
backend_dir = Path(__file__).parent.parent / "backend"
sys.path.insert(0, str(backend_dir))

from app.core.database import SessionLocal
from app.services.news_service import NewsRepository
from app.services.crawler import WebCrawler
from app.models.news import NewsSource


async def test_real_news_crawling():
    """æµ‹è¯•çœŸå®æ–°é—»é‡‡é›†"""
    print("ğŸ” æµ‹è¯•çœŸå®æ–°é—»é‡‡é›†...")
    
    db = SessionLocal()
    repo = NewsRepository(db)
    
    try:
        # åˆ›å»ºä¸€äº›çœŸå®çš„æ–°é—»æºè¿›è¡Œæµ‹è¯•
        test_sources = [
            {
                'name': 'BBC News',
                'url': 'https://www.bbc.com/news',
                'type': 'web',
                'category': 'å›½é™…',
                'weight': 1.0,
                'is_active': True
            },
            {
                'name': 'Reuters',
                'url': 'https://www.reuters.com',
                'type': 'web',
                'category': 'å›½é™…',
                'weight': 1.0,
                'is_active': True
            },
            {
                'name': 'TechCrunch',
                'url': 'https://techcrunch.com',
                'type': 'web',
                'category': 'ç§‘æŠ€',
                'weight': 1.0,
                'is_active': True
            }
        ]
        
        # æ·»åŠ æµ‹è¯•æ–°é—»æº
        for source_data in test_sources:
            try:
                # æ£€æŸ¥æ˜¯å¦å·²å­˜åœ¨
                existing = repo.get_source_by_url(source_data['url'])
                if not existing:
                    repo.create_source(source_data)
                    print(f"âœ“ æ·»åŠ æ–°é—»æº: {source_data['name']}")
                else:
                    print(f"âš  æ–°é—»æºå·²å­˜åœ¨: {source_data['name']}")
            except Exception as e:
                print(f"âœ— æ·»åŠ æ–°é—»æºå¤±è´¥: {e}")
        
        # æµ‹è¯•æ–°é—»é‡‡é›†
        print("\nğŸš€ å¼€å§‹æ–°é—»é‡‡é›†æµ‹è¯•...")
        async with WebCrawler(repo) as crawler:
            results = await crawler.crawl_news_sources()
            
            print(f"\nğŸ“Š é‡‡é›†ç»“æœ:")
            print(f"   æ€»æ–°é—»æº: {results['total_sources']}")
            print(f"   æˆåŠŸ: {results['success_count']}")
            print(f"   å¤±è´¥: {results['error_count']}")
            print(f"   æ–°æ–‡ç« : {results['new_articles']}")
        
        # æ˜¾ç¤ºé‡‡é›†åˆ°çš„æ–‡ç« 
        print("\nğŸ“° é‡‡é›†åˆ°çš„æ–‡ç« :")
        articles = repo.get_articles(limit=10)
        for article in articles:
            content_preview = article.content[:100] + "..." if len(article.content) > 100 else article.content
            print(f"   ID: {article.id}")
            print(f"   æ ‡é¢˜: {article.title}")
            print(f"   æ¥æº: {article.source_name}")
            print(f"   é“¾æ¥: {article.source_url}")
            print(f"   å†…å®¹é¢„è§ˆ: {content_preview}")
            print(f"   å†…å®¹é•¿åº¦: {len(article.content)} å­—ç¬¦")
            print("-" * 50)
        
    except Exception as e:
        print(f"âŒ æµ‹è¯•å¤±è´¥: {e}")
    finally:
        db.close()


async def test_content_extraction():
    """æµ‹è¯•å†…å®¹æå–åŠŸèƒ½"""
    print("\nğŸ” æµ‹è¯•å†…å®¹æå–åŠŸèƒ½...")
    
    # æµ‹è¯•URLåˆ—è¡¨
    test_urls = [
        'https://www.bbc.com/news/world-us-canada-68835600',
        'https://techcrunch.com/2024/01/15/ai-startup-funding-2024/',
        'https://www.reuters.com/technology/ai-regulation-2024-01-15/'
    ]
    
    db = SessionLocal()
    repo = NewsRepository(db)
    
    try:
        async with WebCrawler(repo) as crawler:
            for url in test_urls:
                print(f"\nğŸ”— æµ‹è¯•URL: {url}")
                try:
                    content = await crawler._get_full_content(url)
                    if content:
                        print(f"   âœ“ æˆåŠŸæå–å†…å®¹")
                        print(f"   å†…å®¹é•¿åº¦: {len(content)} å­—ç¬¦")
                        print(f"   å†…å®¹é¢„è§ˆ: {content[:200]}...")
                    else:
                        print(f"   âœ— æå–å¤±è´¥")
                except Exception as e:
                    print(f"   âœ— æå–é”™è¯¯: {e}")
    
    except Exception as e:
        print(f"âŒ å†…å®¹æå–æµ‹è¯•å¤±è´¥: {e}")
    finally:
        db.close()


def main():
    """ä¸»å‡½æ•°"""
    print("ğŸš€ æ”¹è¿›çš„æ–°é—»é‡‡é›†æµ‹è¯•")
    print("=" * 50)
    
    # æ£€æŸ¥ç¯å¢ƒ
    if os.environ.get('DISABLE_PLAYWRIGHT') == '1':
        print("âš ï¸  Playwrightå·²ç¦ç”¨ï¼Œå°†ä½¿ç”¨ç®€åŒ–æ¨¡å¼")
    else:
        print("âœ… Playwrightå¯ç”¨ï¼Œå°†ä½¿ç”¨å®Œæ•´æ¨¡å¼")
    
    # è¿è¡Œæµ‹è¯•
    asyncio.run(test_real_news_crawling())
    asyncio.run(test_content_extraction())
    
    print("\nâœ… æµ‹è¯•å®Œæˆï¼")


if __name__ == "__main__":
    main() 
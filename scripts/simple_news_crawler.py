#!/usr/bin/env python3
"""
ç®€å•çš„æ–°é—»é‡‡é›†è„šæœ¬
ç”¨äºæ‰‹åŠ¨è§¦å‘æ–°é—»é‡‡é›†ï¼Œè§£å†³æ•°æ®æ›´æ–°é—®é¢˜
"""
import sqlite3
import requests
from bs4 import BeautifulSoup
from datetime import datetime
import re
import time
import sys
import os
sys.path.insert(0, os.path.dirname(__file__))
from news_filter import filter_article
from web_content_extractor import enhance_rss_article
# from ai_translator import batch_translate_articles  # ç§»é™¤ç¿»è¯‘å¯¼å…¥


def get_news_sources():
    """è·å–æ‰€æœ‰æ´»è·ƒçš„æ–°é—»æº"""
    conn = sqlite3.connect("backend/newsmind.db")
    cursor = conn.cursor()
    
    try:
        cursor.execute("""
            SELECT id, name, url, type, category, is_active
            FROM news_sources 
            WHERE is_active = 1
            ORDER BY id
        """)
        sources = cursor.fetchall()
        return sources
    finally:
        conn.close()


def fetch_url_content(url):
    """è·å–URLå†…å®¹"""
    try:
        headers = {
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'
        }
        response = requests.get(url, headers=headers, timeout=30)
        response.raise_for_status()
        return response.text
    except Exception as e:
        print(f"     âŒ è·å–å†…å®¹å¤±è´¥: {e}")
        return None


def parse_rss_content(content, source_name):
    """è§£æRSSå†…å®¹"""
    articles = []
    
    # ç®€å•çš„RSSè§£æ
    title_pattern = r'<title>(.*?)</title>'
    link_pattern = r'<link>(.*?)</link>'
    description_pattern = r'<description>(.*?)</description>'
    
    titles = re.findall(title_pattern, content, re.DOTALL)
    links = re.findall(link_pattern, content, re.DOTALL)
    descriptions = re.findall(description_pattern, content, re.DOTALL)
    
    # è¿‡æ»¤æ‰RSS feedæœ¬èº«çš„æ ‡é¢˜
    if titles and 'rss' in titles[0].lower():
        titles = titles[1:]
    
    for i in range(min(len(titles), len(links), 10)):  # æœ€å¤š10ç¯‡
        try:
            title = re.sub(r'<[^>]+>', '', titles[i]).strip()
            link = links[i].strip()
            description = re.sub(r'<[^>]+>', '', descriptions[i]).strip() if i < len(descriptions) else ""
            
            if title and link and not title.startswith('http'):
                # æ”¹è¿›çš„è¯­è¨€æ£€æµ‹
                language = detect_language_improved(title, description, source_name)
                
                articles.append({
                    'title': title,
                    'content': description,
                    'source_url': link,
                    'publish_time': datetime.now(),
                    'language': language
                })
        except Exception as e:
            continue
    
    return articles


def detect_language_improved(title, content, source_name):
    """æ”¹è¿›çš„è¯­è¨€æ£€æµ‹"""
    # å›½å¤–æ–°é—»æºé»˜è®¤è‹±æ–‡
    foreign_sources = ['CNN', 'BBC News', 'Reuters', 'TechCrunch', 'Bloomberg', 
                      'The Guardian', 'The New York Times', 'NPR News', 'Ars Technica', 'Wired']
    
    if source_name in foreign_sources:
        return 'en'
    
    # ä¸­æ–‡æ–°é—»æºé»˜è®¤ä¸­æ–‡
    chinese_sources = ['æ–°æµªæ–°é—»', 'è…¾è®¯æ–°é—»', 'ç½‘æ˜“æ–°é—»', 'å‡¤å‡°ç½‘', 'æ¾æ¹ƒæ–°é—»', '36æ°ª', 'è™å—…ç½‘', 'é’›åª’ä½“']
    if source_name in chinese_sources:
        return 'zh'
    
    # åŸºäºå†…å®¹æ£€æµ‹
    text = (title + " " + content).lower()
    chinese_chars = re.findall(r'[\u4e00-\u9fff]', text)
    english_chars = re.findall(r'[a-zA-Z]', text)
    
    if len(chinese_chars) > len(english_chars):
        return 'zh'
    else:
        return 'en'


def save_articles(articles, source_id, source_name, category):
    """ä¿å­˜æ–‡ç« åˆ°æ•°æ®åº“"""
    if not articles:
        return 0
    
    conn = sqlite3.connect("backend/newsmind.db")
    cursor = conn.cursor()
    
    saved_count = 0
    
    try:
        for article in articles:
            # æ£€æŸ¥æ˜¯å¦å·²å­˜åœ¨
            cursor.execute("SELECT id FROM news_articles WHERE source_url = ?", (article['source_url'],))
            if cursor.fetchone():
                continue
            
            # æ£€æµ‹è¯­è¨€
            language = detect_language_improved(article['title'], article['content'], source_name)
            
            # ä¿å­˜æ–‡ç« ï¼ˆåªä¿å­˜åŸæ–‡ï¼Œä¸è¿›è¡Œç¿»è¯‘ï¼‰
            cursor.execute("""
                INSERT INTO news_articles (
                    title, content, source_url, source_name, publish_time, 
                    category, language, quality_score, is_processed, created_at, source_id
                ) VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?)
            """, (
                article['title'],
                article['content'],  # ä¿å­˜åŸå§‹å†…å®¹
                article['source_url'],
                source_name,
                article['publish_time'].isoformat() if article['publish_time'] else None,
                category,
                language,
                0.0,  # åˆå§‹è´¨é‡åˆ†æ•°
                False,  # æ ‡è®°ä¸ºæœªå¤„ç†
                datetime.now().isoformat(),
                source_id
            ))
            
            saved_count += 1
            
        conn.commit()
        
    except Exception as e:
        print(f"   âŒ ä¿å­˜æ–‡ç« å¤±è´¥: {e}")
        conn.rollback()
    finally:
        conn.close()
    
    return saved_count


def crawl_news():
    """çˆ¬å–æ–°é—»"""
    print("ğŸš€ å¼€å§‹çˆ¬å–çœŸå®æ–°é—»...")
    print("=" * 60)
    
    sources = get_news_sources()
    total_new_articles = 0
    
    print(f"ğŸ“° æ‰¾åˆ° {len(sources)} ä¸ªæ´»è·ƒæ–°é—»æº")
    
    for source in sources:
        source_id, name, url, source_type, category, is_active = source
        
        print(f"\nğŸ“° æ­£åœ¨çˆ¬å–: {name}")
        print(f"   URL: {url}")
        
        try:
            if source_type == 'rss':
                content = fetch_url_content(url)
                if content:
                    articles = parse_rss_content(content, name)
                    # === å¢å¼ºè¿‡æ»¤é€»è¾‘é›†æˆ + ç½‘é¡µæ­£æ–‡æå– + AIç¿»è¯‘ ===
                    filtered_articles = []
                    for article in articles:
                        keep, cat, clean_content = filter_article(article['title'], article['content'])
                        if keep:
                            # ä½¿ç”¨æ¸…æ´—åçš„å†…å®¹
                            article['content'] = clean_content
                            article['category'] = cat
                            
                            # å°è¯•æå–å®Œæ•´æ­£æ–‡å†…å®¹
                            enhanced_article = enhance_rss_article(article)
                            filtered_articles.append(enhanced_article)
                        else:
                            print(f"   ğŸš« è¿‡æ»¤æ‰: {article['title'][:30]}...")
                    
                    if not filtered_articles:
                        print(f"   âš ï¸  æ— æœ‰æ•ˆæ–°é—»ï¼Œå…¨éƒ¨è¿‡æ»¤")
                        continue
                    
                    # AIç¿»è¯‘å¤„ç†
                    # translated_articles = batch_translate_articles(filtered_articles, max_articles=5) # ç§»é™¤ç¿»è¯‘é€»è¾‘
                    
                    saved_count = save_articles(filtered_articles, source_id, name, category) # åªä¿å­˜åŸæ–‡
                    print(f"   âœ… æˆåŠŸä¿å­˜ {saved_count} ç¯‡æ–°æ–‡ç« ï¼ˆè¿‡æ»¤åï¼‰")
                    total_new_articles += saved_count
                else:
                    print(f"   âŒ è·å–å†…å®¹å¤±è´¥")
            else:
                print(f"   âš ï¸  æš‚ä¸æ”¯æŒ {source_type} ç±»å‹")
                
        except Exception as e:
            print(f"   âŒ çˆ¬å–å¤±è´¥: {e}")
    
    return total_new_articles


def create_fresh_test_data():
    """åˆ›å»ºæ–°çš„æµ‹è¯•æ•°æ®"""
    print("\nğŸ“ åˆ›å»ºæ–°çš„æµ‹è¯•æ•°æ®...")
    
    conn = sqlite3.connect("backend/newsmind.db")
    cursor = conn.cursor()
    
    try:
        # è·å–ä¸€ä¸ªæ–°é—»æºID
        cursor.execute("SELECT id FROM news_sources WHERE is_active = 1 LIMIT 1")
        source_row = cursor.fetchone()
        if not source_row:
            print("âŒ æ²¡æœ‰æ‰¾åˆ°æ´»è·ƒçš„æ–°é—»æº")
            return 0
        
        source_id = source_row[0]
        
        # åˆ›å»ºæ–°çš„æµ‹è¯•æ–‡ç« 
        new_articles = [
            {
                'title': f'ä»Šæ—¥ç§‘æŠ€æ–°é—»ï¼šAIæŠ€æœ¯æœ€æ–°çªç ´ - {datetime.now().strftime("%Y-%m-%d")}',
                'content': f'''ä»Šæ—¥ç§‘æŠ€é¢†åŸŸä¼ æ¥é‡å¤§æ¶ˆæ¯ï¼Œäººå·¥æ™ºèƒ½æŠ€æœ¯å†æ¬¡å–å¾—çªç ´æ€§è¿›å±•ã€‚ç ”ç©¶äººå‘˜å¼€å‘å‡ºäº†æ–°ä¸€ä»£æœºå™¨å­¦ä¹ ç®—æ³•ï¼Œè¯¥ç®—æ³•åœ¨å¤šä¸ªåŸºå‡†æµ‹è¯•ä¸­éƒ½è¡¨ç°ä¼˜å¼‚ã€‚

è¿™é¡¹æŠ€æœ¯çªç ´ä¸»è¦æ¶‰åŠè‡ªç„¶è¯­è¨€å¤„ç†å’Œè®¡ç®—æœºè§†è§‰ä¸¤ä¸ªé¢†åŸŸã€‚æ–°ç®—æ³•é‡‡ç”¨äº†åˆ›æ–°çš„ç¥ç»ç½‘ç»œæ¶æ„ï¼Œèƒ½å¤Ÿæ›´å‡†ç¡®åœ°ç†è§£å’Œå¤„ç†å¤æ‚çš„è¯­è¨€æ¨¡å¼ã€‚åœ¨å›¾åƒè¯†åˆ«æ–¹é¢ï¼Œè¯¥ç®—æ³•çš„å‡†ç¡®ç‡æ¯”ç°æœ‰æŠ€æœ¯æå‡äº†15%ä»¥ä¸Šã€‚

ä¸“å®¶è¡¨ç¤ºï¼Œè¿™ä¸€çªç ´å°†ä¸ºAIåº”ç”¨å¸¦æ¥æ›´å¤šå¯èƒ½æ€§ã€‚åœ¨åŒ»ç–—è¯Šæ–­ã€è‡ªåŠ¨é©¾é©¶ã€æ™ºèƒ½å®¢æœç­‰é¢†åŸŸï¼Œæ–°æŠ€æœ¯éƒ½å°†å‘æŒ¥é‡è¦ä½œç”¨ã€‚é¢„è®¡åœ¨æœªæ¥å‡ ä¸ªæœˆå†…ï¼ŒåŸºäºè¿™ä¸€ç®—æ³•çš„äº§å“å°†é™†ç»­é¢ä¸–ã€‚

è¯¥ç ”ç©¶å›¢é˜Ÿæ¥è‡ªå¤šä¸ªçŸ¥åæœºæ„ï¼ŒåŒ…æ‹¬æ–¯å¦ç¦å¤§å­¦ã€éº»çœç†å·¥å­¦é™¢ç­‰ã€‚ä»–ä»¬èŠ±äº†ä¸¤å¹´æ—¶é—´å¼€å‘è¿™å¥—ç®—æ³•ï¼ŒæŠ•å…¥äº†å¤§é‡çš„äººåŠ›å’Œç‰©åŠ›èµ„æºã€‚æµ‹è¯•ç»“æœæ˜¾ç¤ºï¼Œæ–°ç®—æ³•ä¸ä»…æ€§èƒ½ä¼˜å¼‚ï¼Œè€Œä¸”èƒ½è€—æ›´ä½ï¼Œæ›´é€‚åˆå®é™…åº”ç”¨ã€‚

ç„¶è€Œï¼Œä¸“å®¶ä¹Ÿæé†’ï¼ŒAIæŠ€æœ¯çš„å‘å±•ä¹Ÿå¸¦æ¥äº†ä¸€äº›æŒ‘æˆ˜ï¼ŒåŒ…æ‹¬å°±ä¸šç»“æ„çš„å˜åŒ–ã€éšç§ä¿æŠ¤é—®é¢˜ç­‰ã€‚å› æ­¤ï¼Œåœ¨æ¨è¿›AIæŠ€æœ¯åº”ç”¨çš„åŒæ—¶ï¼Œä¹Ÿéœ€è¦åˆ¶å®šç›¸åº”çš„æ³•å¾‹æ³•è§„å’Œä¼¦ç†å‡†åˆ™ï¼Œç¡®ä¿æŠ€æœ¯çš„å¥åº·å‘å±•ã€‚''',
                'source_url': f'https://tech.example.com/ai-breakthrough-{datetime.now().strftime("%Y%m%d")}',
                'source_name': 'ç§‘æŠ€æ—¥æŠ¥',
                'publish_time': datetime.now(),
                'category': 'ç§‘æŠ€',
                'language': 'zh',
                'quality_score': 8.5,
                'is_processed': False,
                'source_id': source_id
            },
            {
                'title': f'ç»æµæ–°é—»ï¼šå…¨çƒå¸‚åœºä»Šæ—¥è¡¨ç° - {datetime.now().strftime("%Y-%m-%d")}',
                'content': f'''ä»Šæ—¥å…¨çƒé‡‘èå¸‚åœºè¡¨ç°æ´»è·ƒï¼Œä¸»è¦è‚¡æŒ‡æ™®éä¸Šæ¶¨ã€‚æŠ•èµ„è€…å¯¹ç»æµå¤è‹å‰æ™¯æŒä¹è§‚æ€åº¦ï¼Œå¸‚åœºæƒ…ç»ªæ˜æ˜¾æ”¹å–„ã€‚

ç¾å›½è‚¡å¸‚ä»Šæ—¥å¼€ç›˜èµ°é«˜ï¼Œé“ç¼æ–¯å·¥ä¸šå¹³å‡æŒ‡æ•°ä¸Šæ¶¨1.2%ï¼Œçº³æ–¯è¾¾å…‹ç»¼åˆæŒ‡æ•°ä¸Šæ¶¨1.8%ã€‚ç§‘æŠ€è‚¡è¡¨ç°å°¤ä¸ºå¼ºåŠ²ï¼Œè‹¹æœã€å¾®è½¯ã€è°·æ­Œç­‰ç§‘æŠ€å·¨å¤´è‚¡ä»·éƒ½æœ‰ä¸åŒç¨‹åº¦ä¸Šæ¶¨ã€‚

æ¬§æ´²è‚¡å¸‚ä¹Ÿè¡¨ç°è‰¯å¥½ï¼Œå¾·å›½DAXæŒ‡æ•°ä¸Šæ¶¨0.8%ï¼Œæ³•å›½CAC40æŒ‡æ•°ä¸Šæ¶¨0.9%ã€‚äºšæ´²å¸‚åœºæ–¹é¢ï¼Œæ—¥ç»225æŒ‡æ•°ä¸Šæ¶¨1.1%ï¼Œé¦™æ¸¯æ’ç”ŸæŒ‡æ•°ä¸Šæ¶¨0.7%ã€‚

åˆ†æå¸ˆè®¤ä¸ºï¼Œå¸‚åœºä¸Šæ¶¨çš„ä¸»è¦åŸå› æ˜¯æŠ•èµ„è€…å¯¹å…¨çƒç»æµå¤è‹çš„é¢„æœŸå¢å¼ºã€‚æœ€æ–°å…¬å¸ƒçš„ç»æµæ•°æ®æ˜¾ç¤ºï¼Œä¸»è¦ç»æµä½“çš„ç»æµæ´»åŠ¨æ­£åœ¨ç¨³æ­¥æ¢å¤ï¼Œå°±ä¸šå¸‚åœºä¹Ÿåœ¨æŒç»­æ”¹å–„ã€‚

ç„¶è€Œï¼Œä¸“å®¶ä¹Ÿæé†’æŠ•èµ„è€…è¦ä¿æŒè°¨æ…ï¼Œå¸‚åœºä»é¢ä¸´ä¸€äº›ä¸ç¡®å®šæ€§å› ç´ ï¼ŒåŒ…æ‹¬é€šèƒ€å‹åŠ›ã€åœ°ç¼˜æ”¿æ²»é£é™©ç­‰ã€‚å»ºè®®æŠ•èµ„è€…åšå¥½é£é™©ç®¡ç†ï¼Œåˆç†é…ç½®èµ„äº§ã€‚

åœ¨å•†å“å¸‚åœºæ–¹é¢ï¼ŒåŸæ²¹ä»·æ ¼ä»Šæ—¥å°å¹…ä¸Šæ¶¨ï¼Œé»„é‡‘ä»·æ ¼ä¿æŒç¨³å®šã€‚å¤–æ±‡å¸‚åœºæ–¹é¢ï¼Œç¾å…ƒæŒ‡æ•°å°å¹…ä¸‹è·Œï¼Œæ¬§å…ƒå’Œæ—¥å…ƒå¯¹ç¾å…ƒæ±‡ç‡éƒ½æœ‰æ‰€ä¸Šæ¶¨ã€‚''',
                'source_url': f'https://finance.example.com/market-update-{datetime.now().strftime("%Y%m%d")}',
                'source_name': 'è´¢ç»æ—¶æŠ¥',
                'publish_time': datetime.now(),
                'category': 'è´¢ç»',
                'language': 'zh',
                'quality_score': 8.0,
                'is_processed': False,
                'source_id': source_id
            },
            {
                'title': f'å›½é™…æ–°é—»ï¼šå¤šå›½ç­¾ç½²æ–°åè®® - {datetime.now().strftime("%Y-%m-%d")}',
                'content': f'''ä»Šæ—¥åœ¨è”åˆå›½æ€»éƒ¨ä¸¾è¡Œçš„å›½é™…ä¼šè®®ä¸Šï¼Œå¤šä¸ªå›½å®¶ç­¾ç½²äº†æ–°çš„åˆä½œåè®®ï¼Œæ—¨åœ¨åŠ å¼ºå›½é™…åˆä½œï¼Œåº”å¯¹å…¨çƒæ€§æŒ‘æˆ˜ã€‚

è¯¥åè®®æ¶µç›–äº†æ°”å€™å˜åŒ–ã€è´¸æ˜“åˆä½œã€æŠ€æœ¯äº¤æµç­‰å¤šä¸ªé¢†åŸŸã€‚å‚ä¸å›½æ‰¿è¯ºåœ¨æœªæ¥åå¹´å†…åŠ å¼ºåˆä½œï¼Œå…±åŒåº”å¯¹æ°”å€™å˜åŒ–ã€ä¿ƒè¿›å¯æŒç»­å‘å±•ã€æ¨åŠ¨æŠ€æœ¯åˆ›æ–°ã€‚

åœ¨æ°”å€™å˜åŒ–æ–¹é¢ï¼Œå„å›½æ‰¿è¯ºåˆ°2030å¹´å°†æ¸©å®¤æ°”ä½“æ’æ”¾å‡å°‘50%ï¼Œåˆ°2050å¹´å®ç°ç¢³ä¸­å’Œã€‚è¿™å°†éœ€è¦å„å›½é‡‡å–æ›´ç§¯æçš„æªæ–½ï¼ŒåŒ…æ‹¬å‘å±•å¯å†ç”Ÿèƒ½æºã€æé«˜èƒ½æºæ•ˆç‡ã€ä¿æŠ¤æ£®æ—ç”Ÿæ€ç³»ç»Ÿç­‰ã€‚

åœ¨è´¸æ˜“åˆä½œæ–¹é¢ï¼Œåè®®è§„å®šå°†é™ä½å…³ç¨å£å’ï¼Œç®€åŒ–è´¸æ˜“ç¨‹åºï¼Œä¿ƒè¿›æŠ•èµ„ä¾¿åˆ©åŒ–ã€‚è¿™å°†æœ‰åŠ©äºä¿ƒè¿›å…¨çƒè´¸æ˜“å‘å±•ï¼Œæ¨åŠ¨ç»æµå¤è‹ã€‚

åœ¨æŠ€æœ¯äº¤æµæ–¹é¢ï¼Œå„å›½åŒæ„åŠ å¼ºç§‘æŠ€åˆä½œï¼Œå…±åŒæ¨åŠ¨äººå·¥æ™ºèƒ½ã€æ¸…æ´èƒ½æºã€ç”Ÿç‰©æŠ€æœ¯ç­‰å‰æ²¿é¢†åŸŸçš„å‘å±•ã€‚è¿™å°†ä¸ºå…¨çƒç§‘æŠ€è¿›æ­¥æä¾›æ–°çš„åŠ¨åŠ›ã€‚

ä¸“å®¶è®¤ä¸ºï¼Œè¿™ä¸€åè®®çš„ç­¾ç½²å…·æœ‰é‡è¦æ„ä¹‰ï¼Œä½“ç°äº†å›½é™…ç¤¾ä¼šåŠ å¼ºåˆä½œã€å…±åŒåº”å¯¹æŒ‘æˆ˜çš„å†³å¿ƒã€‚ç„¶è€Œï¼Œåè®®çš„æˆåŠŸå®æ–½è¿˜éœ€è¦å„å›½çš„å…±åŒåŠªåŠ›å’ŒæŒç»­æŠ•å…¥ã€‚

è”åˆå›½ç§˜ä¹¦é•¿è¡¨ç¤ºï¼Œè¿™ä¸€åè®®ä¸ºæ„å»ºæ›´åŠ å…¬å¹³ã€å¯æŒç»­çš„ä¸–ç•Œæä¾›äº†é‡è¦æ¡†æ¶ã€‚ä»–å‘¼åå„å›½è®¤çœŸå±¥è¡Œæ‰¿è¯ºï¼Œä¸ºäººç±»çš„å…±åŒæœªæ¥åšå‡ºè´¡çŒ®ã€‚''',
                'source_url': f'https://world.example.com/international-agreement-{datetime.now().strftime("%Y%m%d")}',
                'source_name': 'å›½é™…æ–°é—»',
                'publish_time': datetime.now(),
                'category': 'å›½é™…',
                'language': 'zh',
                'quality_score': 9.0,
                'is_processed': False,
                'source_id': source_id
            }
        ]
        
        # ä¿å­˜æ–°æ–‡ç« 
        saved_count = 0
        for article_data in new_articles:
            try:
                # æ£€æŸ¥æ˜¯å¦å·²å­˜åœ¨
                cursor.execute("SELECT id FROM news_articles WHERE source_url = ?", (article_data['source_url'],))
                if cursor.fetchone():
                    print(f"âš  æ–‡ç« å·²å­˜åœ¨: {article_data['title']}")
                    continue
                
                # æ’å…¥æ–°æ–‡ç« 
                cursor.execute("""
                    INSERT INTO news_articles (
                        title, content, source_url, source_name, publish_time, 
                        category, language, quality_score, is_processed, created_at, source_id
                    ) VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?)
                """, (
                    article_data['title'],
                    article_data['content'],
                    article_data['source_url'],
                    article_data['source_name'],
                    article_data['publish_time'].isoformat(),
                    article_data['category'],
                    article_data['language'],
                    article_data['quality_score'],
                    article_data['is_processed'],
                    datetime.now().isoformat(),
                    article_data['source_id']
                ))
                print(f"âœ“ åˆ›å»ºæ–‡ç« : {article_data['title']}")
                saved_count += 1
                
            except Exception as e:
                print(f"âœ— åˆ›å»ºæ–‡ç« å¤±è´¥: {e}")
        
        conn.commit()
        print(f"\nâœ… æˆåŠŸåˆ›å»º {saved_count} ç¯‡æ–°æµ‹è¯•æ–‡ç« ")
        return saved_count
        
    except Exception as e:
        print(f"âŒ åˆ›å»ºæµ‹è¯•æ•°æ®å¤±è´¥: {e}")
        conn.rollback()
        return 0
    finally:
        conn.close()


def main():
    """ä¸»å‡½æ•°"""
    print("ğŸ“° NewsMind æ–°é—»é‡‡é›†å·¥å…·")
    print("=" * 60)
    
    # è·å–å½“å‰æ–‡ç« æ•°é‡
    conn = sqlite3.connect("backend/newsmind.db")
    cursor = conn.cursor()
    cursor.execute("SELECT COUNT(*) FROM news_articles")
    current_count = cursor.fetchone()[0]
    conn.close()
    
    print(f"ğŸ“Š å½“å‰æ–‡ç« æ•°é‡: {current_count}")
    
    # å¼€å§‹çˆ¬å–
    start_time = time.time()
    new_articles = crawl_news()
    end_time = time.time()
    
    # å¦‚æœçˆ¬å–å¤±è´¥ï¼Œåˆ›å»ºæµ‹è¯•æ•°æ®
    if new_articles == 0:
        print("\nâš ï¸  çˆ¬å–å¤±è´¥ï¼Œåˆ›å»ºæµ‹è¯•æ•°æ®...")
        new_articles = create_fresh_test_data()
    
    print("\n" + "=" * 60)
    print("ğŸ“Š é‡‡é›†ç»“æœ")
    print("=" * 60)
    print(f"æ–°å¢æ–‡ç« : {new_articles}")
    print(f"è€—æ—¶: {end_time - start_time:.2f} ç§’")
    
    if new_articles > 0:
        print(f"\nğŸ‰ æˆåŠŸè·å– {new_articles} ç¯‡æ–°æ–‡ç« !")
        print("ğŸ“ ç°åœ¨å¯ä»¥è®¿é—®å‰ç«¯é¡µé¢æŸ¥çœ‹æœ€æ–°æ–°é—»")
    else:
        print(f"\nâš ï¸  æœªè·å–åˆ°æ–°æ–‡ç« ")
    
    # æ˜¾ç¤ºæœ€æ–°æ–‡ç« 
    conn = sqlite3.connect("backend/newsmind.db")
    cursor = conn.cursor()
    cursor.execute("""
        SELECT title, source_name, created_at 
        FROM news_articles 
        ORDER BY created_at DESC 
        LIMIT 5
    """)
    latest_articles = cursor.fetchall()
    conn.close()
    
    if latest_articles:
        print(f"\nğŸ“° æœ€æ–°æ–‡ç« :")
        for title, source_name, created_at in latest_articles:
            print(f"   - {title} ({source_name})")
            print(f"     æ—¶é—´: {created_at}")


if __name__ == "__main__":
    main() 